{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(folder_path):\n",
    "    individuals = ['S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'S10', 'S11', 'S13', 'S14', 'S15', 'S16', 'S17']  # List of individual folder names\n",
    "    sampling_rate_original = 4  # Original sampling rate\n",
    "    sampling_rate_target = 1  # Target sampling rate\n",
    "    dfs = []\n",
    "    \n",
    "    for idx, individual in enumerate(individuals):\n",
    "        individual_path = os.path.join(folder_path, individual)\n",
    "        if not os.path.isdir(individual_path):\n",
    "            continue  # Skip if the folder doesn't exist\n",
    "\n",
    "        hr_path = os.path.join(individual_path, 'E4_Data', 'HR.csv')\n",
    "        temp_path = os.path.join(individual_path, 'E4_Data', 'TEMP.csv')\n",
    "\n",
    "        if not (os.path.isfile(hr_path) and os.path.isfile(temp_path)):\n",
    "            continue  \n",
    "\n",
    "        # HR signal\n",
    "        hr_df = pd.read_csv(hr_path, names=['hr'], skiprows=2)\n",
    "        hr_signal = hr_df['hr'].values\n",
    "        hr_smoothed = pd.Series(hr_signal).rolling(3).mean().values\n",
    "\n",
    "        # Temp signal\n",
    "        temp_df = pd.read_csv(temp_path, names=['temp'], skiprows=2)\n",
    "        temp_signal = temp_df['temp'].values\n",
    "        temp_resampled = signal.resample(temp_signal, int(len(temp_signal) * sampling_rate_target / sampling_rate_original))\n",
    "        temp_smoothed = pd.Series(temp_resampled).rolling(3).mean().values\n",
    "\n",
    "        # Time \n",
    "        start_time = pd.to_datetime('00:00', format='%M:%S')\n",
    "        time_hr = pd.Series([start_time + timedelta(seconds=(i / sampling_rate_target)) for i in range(len(hr_smoothed))])\n",
    "        time_temp = pd.Series([start_time + timedelta(seconds=(i / sampling_rate_target)) for i in range(len(temp_smoothed))])\n",
    "\n",
    "        # Create dataframes for 'bvp' and 'temp' signals with time vectors and ID column\n",
    "        hr_df_processed = pd.DataFrame({'ID': [idx] * len(hr_smoothed), 'Time': time_hr, 'hr': hr_smoothed})\n",
    "        temp_df_processed = pd.DataFrame({'ID': [idx] * len(temp_smoothed), 'Time': time_temp, 'temp': temp_smoothed})\n",
    "\n",
    "        # Merge the dataframes based on the 'ID' and 'Time' columns\n",
    "        final_df = pd.merge(hr_df_processed, temp_df_processed, on=['ID', 'Time'], how='outer')\n",
    "\n",
    "        # Handle missing values, if any\n",
    "        final_df = final_df.ffill().bfill()  # Forward-fill and backward-fill missing values\n",
    "\n",
    "        # Convert hours to minutes \n",
    "        final_df['Time'] = final_df['Time'].dt.hour * 60 + final_df['Time'].dt.minute\n",
    "\n",
    "        # Fill with labels\n",
    "        final_df['labels'] = 999 # Fill with placeholder for transition \n",
    "        quest = pd.read_csv(os.path.join(individual_path, 'quest.csv'), delimiter= ';')\n",
    "        labels = quest.columns\n",
    "\n",
    "        final_df.loc[(final_df['Time'] >= quest.iloc[0, 1]) & (final_df['Time'] <= quest.iloc[1, 1]), 'labels'] = labels[1]\n",
    "        final_df.loc[(final_df['Time'] >= quest.iloc[0, 2]) & (final_df['Time'] <= quest.iloc[1, 2]), 'labels'] = labels[2]\n",
    "        final_df.loc[(final_df['Time'] >= quest.iloc[0, 3]) & (final_df['Time'] <= quest.iloc[1, 3]), 'labels'] = labels[3]\n",
    "        final_df.loc[(final_df['Time'] >= quest.iloc[0, 4]) & (final_df['Time'] <= quest.iloc[1, 4]), 'labels'] = labels[4]\n",
    "        final_df.loc[(final_df['Time'] >= quest.iloc[0, 5]) & (final_df['Time'] <= quest.iloc[1, 5]), 'labels'] = labels[5]\n",
    "\n",
    "        start_time = pd.to_datetime('2023-07-13 00:00', format='%Y-%m-%d %M:%S')\n",
    "        final_df['Time'] = pd.Series([start_time + timedelta(seconds=(i / sampling_rate_target)) for i in range(len(final_df))])\n",
    "\n",
    "        dfs.append(final_df)\n",
    "\n",
    "    # Save the merged dataframe to a CSV file\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "    output_file_path = os.path.join(folder_path, 'final_df.csv')\n",
    "    merged_df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Merged dataset saved as {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset saved as C:/Users/aless/OneDrive - Università degli Studi di Catania/tesi/dataset\\final_df.csv\n"
     ]
    }
   ],
   "source": [
    "folder_path = 'C:/Users/aless/OneDrive - Università degli Studi di Catania/tesi/dataset'\n",
    "process_data(folder_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
